{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nl2tU6kL8Ot3"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4A9uU1WloQ2"
   },
   "source": [
    "# Data Loader Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cq3KEUaVo1o3"
   },
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "\n",
    "    # ORIGINAL\n",
    "    # train = np.loadtxt(path+'movielens_100k_umin_clipbase', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    # test = np.loadtxt(path+'movielens_100k_umin_cliptest', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    # total = np.concatenate((train, test), axis=0)\n",
    "\n",
    "    # NEW\n",
    "    total = np.loadtxt(path+'u.data', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    mask = np.random.rand(len(total)) < 0.9\n",
    "    train = total[mask]\n",
    "    test = total[~mask]\n",
    "\n",
    "\n",
    "    n_u = np.unique(total[:,0]).size  # num of users\n",
    "    n_m = np.unique(total[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "\n",
    "    for i in range(n_train):\n",
    "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
    "\n",
    "    for i in range(n_test):\n",
    "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P3e8Xg3us8g7"
   },
   "outputs": [],
   "source": [
    "def load_data_1m(path='./', delimiter='::', frac=0.1, seed=1234):\n",
    "\n",
    "    tic = time()\n",
    "    print('reading data...')\n",
    "    data = np.loadtxt(path+'ratings.dat', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    print('taken', time() - tic, 'seconds')\n",
    "\n",
    "    n_u = np.unique(data[:,0]).size  # num of users\n",
    "    n_m = np.unique(data[:,1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    udict = {}\n",
    "    for i, u in enumerate(np.unique(data[:,0]).tolist()):\n",
    "        udict[u] = i\n",
    "    mdict = {}\n",
    "    for i, m in enumerate(np.unique(data[:,1]).tolist()):\n",
    "        mdict[m] = i\n",
    "\n",
    "    # np.random.seed(seed)\n",
    "    idx = np.arange(n_r)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "\n",
    "    for i in range(n_r):\n",
    "        u_id = data[idx[i], 0]\n",
    "        m_id = data[idx[i], 1]\n",
    "        r = data[idx[i], 2]\n",
    "\n",
    "        if i < int(frac * n_r):\n",
    "            test_r[mdict[m_id], udict[u_id]] = r\n",
    "        else:\n",
    "            train_r[mdict[m_id], udict[u_id]] = r\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_r - int(frac * n_r)))\n",
    "    print('num of test ratings: {}'.format(int(frac * n_r)))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def load_data_tripadvisor(path='./', frac=0.1, seed=1234):\n",
    "\n",
    "    tic = time()\n",
    "    print('reading data...')\n",
    "    # Load reviews.pkl\n",
    "\n",
    "\n",
    "    \n",
    "    df = pd.read_pickle(path+ \"reviews.pkl\")\n",
    "\n",
    "    print(\"b\")\n",
    "    df = df[[\"userId\", \"restaurantId\", \"rating\"]]\n",
    "    df.columns = [\"user_id\", \"item_id\", \"rating\"]\n",
    "\n",
    "    print(\"c\")\n",
    "    # Drop items with less than 100 ratings and users with less than 20 ratings\n",
    "    # Remove repeated user-item pairs\n",
    "    df = df.drop_duplicates(subset=[\"user_id\", \"item_id\"], keep=\"first\")\n",
    "\n",
    "    # Create new user and item ids ( userId's are strings, restaurantId's are not continuous)\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(\"category\").cat.codes\n",
    "    df[\"item_id\"] = df[\"item_id\"].astype(\"category\").cat.codes\n",
    "\n",
    "    # Remove items with less than 10 ratings\n",
    "    postsPerItem = df.groupby(['item_id']).size()\n",
    "    df = df[np.in1d(df.item_id, postsPerItem[postsPerItem >= 10].index)]\n",
    "\n",
    "    # Remove users with less than 10 ratings\n",
    "    postsPerUser = df.groupby(['user_id']).size()\n",
    "    df = df[np.in1d(df.user_id, postsPerUser[postsPerUser >= 10].index)]\n",
    "\n",
    "    print(\"e\")\n",
    "\n",
    "    # Remove NA values\n",
    "    df = df.dropna()\n",
    "\n",
    "    print(\"f\")\n",
    "    # Create new user and item ids ( userId's are strings, restaurantId's are not continuous)\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(\"category\").cat.codes\n",
    "    df[\"item_id\"] = df[\"item_id\"].astype(\"category\").cat.codes\n",
    "\n",
    "    print(\"g\")\n",
    "\n",
    "    df[\"rating\"] = df[\"rating\"] / 10\n",
    "\n",
    "    print(\"h\")\n",
    "\n",
    "    data = df.values\n",
    "\n",
    "    print('taken', time() - tic, 'seconds')\n",
    "\n",
    "    print(\"Constructing data matrix...\")\n",
    "    n_u = np.unique(data[:,0]).size  # num of users\n",
    "    n_m = np.unique(data[:,1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    udict = {}\n",
    "    for i, u in enumerate(np.unique(data[:,0]).tolist()):\n",
    "        udict[u] = i\n",
    "    mdict = {}\n",
    "    for i, m in enumerate(np.unique(data[:,1]).tolist()):\n",
    "        mdict[m] = i\n",
    "\n",
    "    # np.random.seed(seed)\n",
    "    idx = np.arange(n_r)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "\n",
    "    for i in range(n_r):\n",
    "        u_id = data[idx[i], 0]\n",
    "        m_id = data[idx[i], 1]\n",
    "        r = data[idx[i], 2]\n",
    "\n",
    "        if i < int(frac * n_r):\n",
    "            test_r[mdict[m_id], udict[u_id]] = r\n",
    "        else:\n",
    "            train_r[mdict[m_id], udict[u_id]] = r\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_r - int(frac * n_r)))\n",
    "    print('num of test ratings: {}'.format(int(frac * n_r)))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data_gdsc1(path=\"./\", frac=0.1, seed=1234):\n",
    "\n",
    "    global min_rating\n",
    "\n",
    "    df = pd.read_csv(path + \"GDSC1_processed.csv\")\n",
    "\n",
    "    df = df.sample(frac=.3).reset_index(drop=True)\n",
    "\n",
    "    min_rating = df[\"rating\"].min()\n",
    "\n",
    "    df[\"rating\"] = df[\"rating\"] - min_rating\n",
    "\n",
    "    data = df.values\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    print(\"Constructing data matrix...\")\n",
    "    n_u = np.unique(data[:, 0]).size  # num of users\n",
    "    n_m = np.unique(data[:, 1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    udict = {}\n",
    "    for i, u in enumerate(np.unique(data[:, 0]).tolist()):\n",
    "        udict[u] = i\n",
    "    mdict = {}\n",
    "    for i, m in enumerate(np.unique(data[:, 1]).tolist()):\n",
    "        mdict[m] = i\n",
    "\n",
    "    # np.random.seed(seed)\n",
    "    idx = np.arange(n_r)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "    test_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "\n",
    "    for i in range(n_r):\n",
    "        u_id = data[idx[i], 0]\n",
    "        m_id = data[idx[i], 1]\n",
    "        r = data[idx[i], 2]\n",
    "\n",
    "        if i < int(frac * n_r):\n",
    "            test_r[mdict[m_id], udict[u_id]] = r\n",
    "        else:\n",
    "            train_r[mdict[m_id], udict[u_id]] = r\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype(\"float32\")  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype(\"float32\")\n",
    "\n",
    "    print(\"data matrix loaded\")\n",
    "    print(\"num of users: {}\".format(n_u))\n",
    "    print(\"num of movies: {}\".format(n_m))\n",
    "    print(\"num of training ratings: {}\".format(n_r - int(frac * n_r)))\n",
    "    print(\"num of test ratings: {}\".format(int(frac * n_r)))\n",
    "    print(\"minimum and maximum ratings: {}, {}\".format(train_r.min(), train_r.max()))\n",
    "    print(\"minimum and maximum ratings: {}, {}\".format(test_r.min(), test_r.max()))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data_diabetes(path=\"./\", frac=0.1, seed=1234):\n",
    "\n",
    "    global min_rating\n",
    "\n",
    "    df = pd.read_csv(path + \"diabetes_processed.csv\")\n",
    "\n",
    "    data = df.values\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    print(\"Constructing data matrix...\")\n",
    "    n_u = np.unique(data[:, 0]).size  # num of users\n",
    "    n_m = np.unique(data[:, 2]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    print(data[:,1].min(), data[:,1].max())\n",
    "\n",
    "    udict = {}\n",
    "    for i, u in enumerate(np.unique(data[:, 0]).tolist()):\n",
    "        udict[u] = i\n",
    "    mdict = {}\n",
    "    for i, m in enumerate(np.unique(data[:, 2]).tolist()):\n",
    "        mdict[m] = i\n",
    "\n",
    "    # np.random.seed(seed)\n",
    "    idx = np.arange(n_r)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "    test_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "\n",
    "    for i in range(n_r):\n",
    "        u_id = data[idx[i], 0]\n",
    "        m_id = data[idx[i], 2]\n",
    "        r = data[idx[i], 1]\n",
    "\n",
    "        if i < int(frac * n_r):\n",
    "            test_r[mdict[m_id], udict[u_id]] = r\n",
    "        else:\n",
    "            train_r[mdict[m_id], udict[u_id]] = r\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype(\"float32\")  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype(\"float32\")\n",
    "\n",
    "    print(\"data matrix loaded\")\n",
    "    print(\"num of users: {}\".format(n_u))\n",
    "    print(\"num of movies: {}\".format(n_m))\n",
    "    print(\"num of training ratings: {}\".format(n_r - int(frac * n_r)))\n",
    "    print(\"num of test ratings: {}\".format(int(frac * n_r)))\n",
    "    print(\"minimum and maximum ratings: {}, {}\".format(train_r.min(), train_r.max()))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data_dot(path=\"./\", frac=0.1, seed=1234):\n",
    "\n",
    "    df = pd.read_csv(path + \"dot_2023_processed.csv\")\n",
    "\n",
    "    data = df.values\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    print(\"Constructing data matrix...\")\n",
    "    n_u = np.unique(data[:, 0]).size  # num of users\n",
    "    n_m = np.unique(data[:,1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    print(data[:, 2].min(), data[:, 2].max())\n",
    "\n",
    "    udict = {}\n",
    "    for i, u in enumerate(np.unique(data[:, 0]).tolist()):\n",
    "        udict[u] = i\n",
    "    mdict = {}\n",
    "    for i, m in enumerate(np.unique(data[:, 1]).tolist()):\n",
    "        mdict[m] = i\n",
    "\n",
    "    # np.random.seed(seed)\n",
    "    idx = np.arange(n_r)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "    test_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "\n",
    "    for i in range(n_r):\n",
    "        u_id = data[idx[i], 0]\n",
    "        m_id = data[idx[i], 1]\n",
    "        r = data[idx[i], 2]\n",
    "\n",
    "        if i < int(frac * n_r):\n",
    "            test_r[mdict[m_id], udict[u_id]] = r\n",
    "        else:\n",
    "            train_r[mdict[m_id], udict[u_id]] = r\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype(\"float32\")  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype(\"float32\")\n",
    "\n",
    "    print(\"data matrix loaded\")\n",
    "    print(\"num of users: {}\".format(n_u))\n",
    "    print(\"num of movies: {}\".format(n_m))\n",
    "    print(\"num of training ratings: {}\".format(n_r - int(frac * n_r)))\n",
    "    print(\"num of test ratings: {}\".format(int(frac * n_r)))\n",
    "    print(\"minimum and maximum ratings: {}, {}\".format(train_r.min(), train_r.max()))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7rMjcbLvhtRs"
   },
   "outputs": [],
   "source": [
    "def load_matlab_file(path_file, name_field):\n",
    "    \n",
    "    db = h5py.File(path_file, 'r')\n",
    "    ds = db[name_field]\n",
    "\n",
    "    try:\n",
    "        if 'ir' in ds.keys():\n",
    "            data = np.asarray(ds['data'])\n",
    "            ir   = np.asarray(ds['ir'])\n",
    "            jc   = np.asarray(ds['jc'])\n",
    "            out  = csc_matrix((data, ir, jc)).astype(np.float32)\n",
    "    except AttributeError:\n",
    "        out = np.asarray(ds).astype(np.float32).T\n",
    "\n",
    "    db.close()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g6pIUrkza2zv"
   },
   "outputs": [],
   "source": [
    "def load_data_monti(path='./'):\n",
    "\n",
    "    M = load_matlab_file(path+'training_test_dataset.mat', 'M')\n",
    "    Otraining = load_matlab_file(path+'training_test_dataset.mat', 'Otraining') * M\n",
    "    Otest = load_matlab_file(path+'training_test_dataset.mat', 'Otest') * M\n",
    "\n",
    "    n_u = M.shape[0]  # num of users\n",
    "    n_m = M.shape[1]  # num of movies\n",
    "    n_train = Otraining[np.where(Otraining)].size  # num of training ratings\n",
    "    n_test = Otest[np.where(Otest)].size  # num of test ratings\n",
    "\n",
    "    train_r = Otraining.T\n",
    "    test_r = Otest.T\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "    \n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data_ctrpv2(path=\"./\", frac=0.1, seed=1234):\n",
    "\n",
    "    df = pd.read_csv(path + \"ctrpv2_processed.csv\")\n",
    "\n",
    "    data = df.values\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    print(\"Constructing data matrix...\")\n",
    "    n_u = np.unique(data[:, 0]).size  # num of users\n",
    "    n_m = np.unique(data[:, 1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    print(data[:,2].min(), data[:, 2].max())\n",
    "\n",
    "    udict = {}\n",
    "    for i, u in enumerate(np.unique(data[:, 0]).tolist()):\n",
    "        udict[u] = i\n",
    "    mdict = {}\n",
    "    for i, m in enumerate(np.unique(data[:, 1]).tolist()):\n",
    "        mdict[m] = i\n",
    "\n",
    "    # np.random.seed(seed)\n",
    "    idx = np.arange(n_r)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "    test_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "\n",
    "    for i in range(n_r):\n",
    "        u_id = data[idx[i], 0]\n",
    "        m_id = data[idx[i], 1]\n",
    "        r = data[idx[i], 2]\n",
    "\n",
    "        if i < int(frac * n_r):\n",
    "            test_r[mdict[m_id], udict[u_id]] = r\n",
    "        else:\n",
    "            train_r[mdict[m_id], udict[u_id]] = r\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype(\"float32\")  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype(\"float32\")\n",
    "\n",
    "    print(\"data matrix loaded\")\n",
    "    print(\"num of users: {}\".format(n_u))\n",
    "    print(\"num of movies: {}\".format(n_m))\n",
    "    print(\"num of training ratings: {}\".format(n_r - int(frac * n_r)))\n",
    "    print(\"num of test ratings: {}\".format(int(frac * n_r)))\n",
    "    print(\"minimum and maximum ratings: {}, {}\".format(train_r.min(), train_r.max()))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data_kiva(path=\"./\", frac=0.1, seed=1234):\n",
    "\n",
    "    global min_rating\n",
    "\n",
    "    df = pd.read_csv(path + \"user_profiles.csv\")\n",
    "\n",
    "    #Create new user and item ids\n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(\"category\").cat.codes\n",
    "    df[\"item_id\"] = df[\"item_id\"].astype(\"category\").cat.codes\n",
    "\n",
    "    data = df.values\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    print(\"Constructing data matrix...\")\n",
    "    n_u = np.unique(data[:, 0]).size  # num of users\n",
    "    n_m = np.unique(data[:,1]).size  # num of movies\n",
    "    n_r = data.shape[0]  # num of ratings\n",
    "\n",
    "    print(data[:, 2].min(), data[:, 2].max())\n",
    "\n",
    "    udict = {}\n",
    "    for i, u in enumerate(np.unique(data[:, 0]).tolist()):\n",
    "        udict[u] = i\n",
    "    mdict = {}\n",
    "    for i, m in enumerate(np.unique(data[:, 1]).tolist()):\n",
    "        mdict[m] = i\n",
    "\n",
    "    # np.random.seed(seed)\n",
    "    idx = np.arange(n_r)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "    test_r = np.zeros((n_m, n_u), dtype=\"float32\")\n",
    "\n",
    "    for i in range(n_r):\n",
    "        u_id = data[idx[i], 0]\n",
    "        m_id = data[idx[i], 1]\n",
    "        r = data[idx[i], 2]\n",
    "\n",
    "        if i < int(frac * n_r):\n",
    "            test_r[mdict[m_id], udict[u_id]] = r\n",
    "        else:\n",
    "            train_r[mdict[m_id], udict[u_id]] = r\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype(\"float32\")  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype(\"float32\")\n",
    "\n",
    "    print(\"data matrix loaded\")\n",
    "    print(\"num of users: {}\".format(n_u))\n",
    "    print(\"num of movies: {}\".format(n_m))\n",
    "    print(\"num of training ratings: {}\".format(n_r - int(frac * n_r)))\n",
    "    print(\"num of test ratings: {}\".format(int(frac * n_r)))\n",
    "    print(\"minimum and maximum ratings: {}, {}\".format(train_r.min(), train_r.max()))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_8kEkg9mlIW"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0fkA1WpmipzF"
   },
   "outputs": [],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = '../../data'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ijlu0lXQioYM"
   },
   "outputs": [],
   "source": [
    "# Select a dataset among 'ML-1M', 'ML-100K', and 'Douban'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "dataset = 'tripadvisor-london'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sJqSSY33mgkw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "b\n",
      "c\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "taken 13.031898498535156 seconds\n",
      "Constructing data matrix...\n",
      "data matrix loaded\n",
      "num of users: 25388\n",
      "num of movies: 12384\n",
      "num of training ratings: 442053\n",
      "num of test ratings: 49117\n"
     ]
    }
   ],
   "source": [
    "# Data Load\n",
    "min_rating = 0\n",
    "\n",
    "try:\n",
    "    if dataset == 'ml-100k':\n",
    "        path = data_path + '/ml-100k/'\n",
    "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')\n",
    "\n",
    "    elif dataset == 'ml-1m':\n",
    "        path = data_path + '/ml-1m/'\n",
    "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_1m(path=path, delimiter='::', frac=0.1, seed=1234)\n",
    "\n",
    "    elif dataset == 'ml-10m':\n",
    "        path = data_path + '/ml-10m/'\n",
    "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_1m(path=path, delimiter='::', frac=0.1, seed=1234)\n",
    "\n",
    "    elif dataset == 'douban-monti':\n",
    "        path = data_path + '/douban-monti/'\n",
    "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_monti(path=path)\n",
    "\n",
    "    elif dataset == 'tripadvisor-london':\n",
    "        path = data_path + '/tripadvisor-london/'\n",
    "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_tripadvisor(path=path)\n",
    "    \n",
    "    elif dataset == 'gdsc1':\n",
    "        path = data_path + '/gdsc1/'\n",
    "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_gdsc1(path=path)\n",
    "\n",
    "    elif dataset == 'uci-diabetes':\n",
    "        path = data_path + '/uci-diabetes/'\n",
    "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_diabetes(path=path)\n",
    "\n",
    "    elif dataset == 'dot_2023':\n",
    "        path = data_path + '/dot_2023/'\n",
    "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_dot(path=path)\n",
    "\n",
    "    elif dataset == 'kiva-ml-17':\n",
    "        path = data_path + '/kiva-ml-17/'\n",
    "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_kiva(path=path)\n",
    "\n",
    "    elif dataset == 'ctrpv2':\n",
    "        path = data_path + '/ctrpv2/'\n",
    "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_ctrpv2(path=path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "except ValueError:\n",
    "    print('Error: Unable to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQMtA9yml-gp"
   },
   "source": [
    "# Hyperparameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nGCdp_FlobOK"
   },
   "outputs": [],
   "source": [
    "# Common hyperparameter settings\n",
    "n_hid = 500\n",
    "n_dim = 5\n",
    "n_layers = 2\n",
    "gk_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "344bwGB0cWXp"
   },
   "outputs": [],
   "source": [
    "# Different hyperparameter settings for each dataset\n",
    "if dataset in [\"ml-100k\", \"kiva-ml-17\", \"dot_2023\", \"tripadvisor-london\"]:\n",
    "    lambda_2 = 20.  # l2 regularisation\n",
    "    lambda_s = 0.006\n",
    "    iter_p = 5  # optimisation\n",
    "    iter_f = 5\n",
    "    epoch_p = 30  # training epoch\n",
    "    epoch_f = 60\n",
    "    dot_scale = 1  # scaled dot product\n",
    "\n",
    "elif dataset in [\n",
    "    \"ml-1m\",\n",
    "    \"gdsc1\",\n",
    "    \"ctrpv2\"\n",
    "]:\n",
    "    lambda_2 = 70.\n",
    "    lambda_s = 0.018\n",
    "    iter_p = 50\n",
    "    iter_f = 10\n",
    "    epoch_p = 20\n",
    "    epoch_f = 30\n",
    "    dot_scale = 0.5\n",
    "\n",
    "elif dataset == 'douban-monti':\n",
    "    lambda_2 = 10.\n",
    "    lambda_s = 0.022\n",
    "    iter_p = 5\n",
    "    iter_f = 5\n",
    "    epoch_p = 20\n",
    "    epoch_f = 60\n",
    "    dot_scale = 2\n",
    "\n",
    "model_params = {\"lambda_2\": lambda_2, \"lambda_s\": lambda_s, \"iter_p\": iter_p, \"iter_f\": iter_f, \"epoch_p\": epoch_p, \"epoch_f\": epoch_f, \"dot_scale\": dot_scale, \"n_hid\": n_hid, \"n_dim\": n_dim, \"n_layers\": n_layers, \"gk_size\": gk_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable GPU\n",
    "tf.compat.v1.config.experimental.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "b94aimX3nAMI"
   },
   "outputs": [],
   "source": [
    "R = tf.placeholder(\"float\", [n_m, n_u])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sWtU4-pmDDT"
   },
   "source": [
    "# Network Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wX2wREO09zde"
   },
   "outputs": [],
   "source": [
    "def local_kernel(u, v):\n",
    "\n",
    "    dist = tf.norm(u - v, ord=2, axis=2)\n",
    "    hat = tf.maximum(0., 1. - dist**2)\n",
    "\n",
    "    return hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "c88l9LYr9175"
   },
   "outputs": [],
   "source": [
    "def kernel_layer(x, n_hid=n_hid, n_dim=n_dim, activation=tf.nn.sigmoid, lambda_s=lambda_s, lambda_2=lambda_2, name=''):\n",
    "\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "        W = tf.get_variable('W', [x.shape[1], n_hid])\n",
    "        n_in = x.get_shape().as_list()[1]\n",
    "        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n",
    "        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n",
    "        b = tf.get_variable('b', [n_hid])\n",
    "\n",
    "    w_hat = local_kernel(u, v)\n",
    "    \n",
    "    sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n",
    "    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n",
    "    \n",
    "    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n",
    "    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n",
    "\n",
    "    W_eff = W * w_hat  # Local kernelised weight matrix\n",
    "    y = tf.matmul(x, W_eff) + b\n",
    "    y = activation(y)\n",
    "\n",
    "    return y, sparse_reg_term + l2_reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rlb95FmRVATa"
   },
   "outputs": [],
   "source": [
    "def global_kernel(input, gk_size, dot_scale):\n",
    "\n",
    "    avg_pooling = tf.reduce_mean(input, axis=1)  # Item (axis=1) based average pooling\n",
    "    avg_pooling = tf.reshape(avg_pooling, [1, -1])\n",
    "    n_kernel = avg_pooling.shape[1].value\n",
    "\n",
    "    conv_kernel = tf.get_variable('conv_kernel', initializer=tf.random.truncated_normal([n_kernel, gk_size**2], stddev=0.1))\n",
    "    gk = tf.matmul(avg_pooling, conv_kernel) * dot_scale  # Scaled dot product\n",
    "    gk = tf.reshape(gk, [gk_size, gk_size, 1, 1])\n",
    "\n",
    "    return gk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jTLi_65XzIbH"
   },
   "outputs": [],
   "source": [
    "def global_conv(input, W):\n",
    "\n",
    "    input = tf.reshape(input, [1, input.shape[0], input.shape[1], 1])\n",
    "    conv2d = tf.nn.relu(tf.nn.conv2d(input, W, strides=[1,1,1,1], padding='SAME'))\n",
    "\n",
    "    return tf.reshape(conv2d, [conv2d.shape[1], conv2d.shape[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8sQCwrSmKG4"
   },
   "source": [
    "# Network Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOtWj1SCo1RW"
   },
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7teUrgWagpW0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "y = R\n",
    "reg_losses = None\n",
    "\n",
    "for i in range(n_layers):\n",
    "    y, reg_loss = kernel_layer(y, name=str(i))\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "\n",
    "pred_p, reg_loss = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
    "reg_losses = reg_losses + reg_loss\n",
    "\n",
    "# L2 loss\n",
    "diff = train_m * (train_r - pred_p)\n",
    "sqE = tf.nn.l2_loss(diff)\n",
    "loss_p = sqE + reg_losses\n",
    "\n",
    "optimizer_p = tf.contrib.opt.ScipyOptimizerInterface(loss_p, options={'disp': True, 'maxiter': iter_p, 'maxcor': 10}, method='L-BFGS-B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IEBsNhNo4Cj"
   },
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "OiTXqnN6zLXQ"
   },
   "outputs": [],
   "source": [
    "y = R\n",
    "reg_losses = None\n",
    "\n",
    "for i in range(n_layers):\n",
    "    y, _ = kernel_layer(y, name=str(i))\n",
    "\n",
    "y_dash, _ = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
    "\n",
    "gk = global_kernel(y_dash, gk_size, dot_scale)  # Global kernel\n",
    "y_hat = global_conv(train_r, gk)  # Global kernel-based rating matrix\n",
    "\n",
    "for i in range(n_layers):\n",
    "    y_hat, reg_loss = kernel_layer(y_hat, name=str(i))\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "\n",
    "pred_f, reg_loss = kernel_layer(y_hat, n_u, activation=tf.identity, name='out')\n",
    "reg_losses = reg_losses + reg_loss\n",
    "\n",
    "# L2 loss\n",
    "diff = train_m * (train_r - pred_f)\n",
    "sqE = tf.nn.l2_loss(diff)\n",
    "loss_f = sqE + reg_losses\n",
    "\n",
    "optimizer_f = tf.contrib.opt.ScipyOptimizerInterface(loss_f, options={'disp': True, 'maxiter': iter_f, 'maxcor': 10}, method='L-BFGS-B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sETwz58aK6y6"
   },
   "source": [
    "# Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vyReXxgac3KH"
   },
   "outputs": [],
   "source": [
    "def dcg_k(score_label, k):\n",
    "    dcg, i = 0., 0\n",
    "    for s in score_label:\n",
    "        if i < k:\n",
    "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
    "            i += 1\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jwsSR-8ZdGWo"
   },
   "outputs": [],
   "source": [
    "def ndcg_k(y_hat, y, k):\n",
    "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
    "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
    "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
    "    norm, i = 0., 0\n",
    "    for s in score_label_:\n",
    "        if i < k:\n",
    "            norm += (2**s[1]-1) / np.log2(2+i)\n",
    "            i += 1\n",
    "    dcg = dcg_k(score_label, k)\n",
    "    return dcg / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "yy9eQS51pbhj"
   },
   "outputs": [],
   "source": [
    "def call_ndcg(y_hat, y):\n",
    "    ndcg_sum, num = 0, 0\n",
    "    y_hat, y = y_hat.T, y.T\n",
    "    n_users = y.shape[0]\n",
    "\n",
    "    for i in range(n_users):\n",
    "        y_hat_i = y_hat[i][np.where(y[i])]\n",
    "        y_i = y[i][np.where(y[i])]\n",
    "\n",
    "        if y_i.shape[0] < 2:\n",
    "            continue\n",
    "\n",
    "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
    "        num += 1\n",
    "\n",
    "    return ndcg_sum / num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXXQjeMxmYEC"
   },
   "source": [
    "# Training and Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Obtain the train and test samples in a Pandas DataFrame\n",
    "# with format user_id (int), movie_id (int), rating (float)\n",
    "# and save them as CSV files\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def extract_test_samples(rating_matrix, mask_matrix):\n",
    "    test_samples = np.where(mask_matrix > 0)\n",
    "    test_samples = np.stack([test_samples[1], test_samples[0]], axis=1)\n",
    "    test_samples = pd.DataFrame(test_samples, columns=['user_id', 'movie_id'])\n",
    "    test_samples['rating'] = rating_matrix[test_samples['movie_id'], test_samples['user_id']]\n",
    "\n",
    "    return test_samples\n",
    "\n",
    "train_samples = extract_test_samples(train_r, train_m)\n",
    "test_samples = extract_test_samples(test_r, test_m)\n",
    "\n",
    "OUTPUTS_PATH = '../../outputs/'\n",
    "MODEL_NAME = 'GLOCAL_K'\n",
    "\n",
    "# Make sure that the directory exists\n",
    "import os\n",
    "os.makedirs(OUTPUTS_PATH + MODEL_NAME, exist_ok=True)\n",
    "\n",
    "print(min_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "UZ35Zoha-Eue"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU device: /device:GPU:0\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 398179.093750\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 55\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 1 test rmse: 1.0544 train rmse: 0.990946\n",
      "Time: 719.229216337204 seconds\n",
      "Time cumulative: 719.229216337204 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "INFO:tensorflow:Optimization terminated with:\n",
      "  Message: b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
      "  Objective function value: 312009.000000\n",
      "  Number of iterations: 50\n",
      "  Number of functions evaluations: 51\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 1.0020207 train rmse: 0.9413342\n",
      "Time: 523.1306517124176 seconds\n",
      "Time cumulative: 1242.3598680496216 seconds\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-d924f9086e76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moptimizer_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_r\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_r\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\contrib\\opt\\python\\training\\external_optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, session, feed_dict, fetches, step_callback, loss_callback, **run_kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mpacked_bounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packed_bounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mstep_callback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep_callback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m         optimizer_kwargs=self.optimizer_kwargs)\n\u001b[0m\u001b[0;32m    208\u001b[0m     var_vals = [\n\u001b[0;32m    209\u001b[0m         \u001b[0mpacked_var_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpacking_slice\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpacking_slice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_packing_slices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\contrib\\opt\\python\\training\\external_optimizer.py\u001b[0m in \u001b[0;36m_minimize\u001b[1;34m(self, initial_val, loss_grad_func, equality_funcs, equality_grad_funcs, inequality_funcs, inequality_grad_funcs, packed_bounds, step_callback, optimizer_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m  \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mminimize_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mminimize_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     message_lines = [\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 618\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\contrib\\opt\\python\\training\\external_optimizer.py\u001b[0m in \u001b[0;36mloss_grad_func_wrapper\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss_grad_func_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m       \u001b[1;31m# SciPy's L-BFGS-B Fortran implementation requires gradients as doubles.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m       \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_grad_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\contrib\\opt\\python\\training\\external_optimizer.py\u001b[0m in \u001b[0;36meval_func\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m       augmented_fetch_vals = session.run(\n\u001b[1;32m--> 278\u001b[1;33m           augmented_fetches, feed_dict=augmented_feed_dict)\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Komi\\Papers\\SUB-COFI\\venv_GLOCAL_K\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
    "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
    "\n",
    "min_clip, max_clip = train_samples[\"rating\"].min(), train_samples[\"rating\"].max()\n",
    "\n",
    "time_cumulative = 0\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Check GPU is available\n",
    "\n",
    "print('GPU available:', tf.test.is_gpu_available())\n",
    "print('GPU device:', tf.test.gpu_device_name())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(epoch_p):\n",
    "        tic = time()\n",
    "        optimizer_p.minimize(sess, feed_dict={R: train_r})\n",
    "        pre = sess.run(pred_p, feed_dict={R: train_r})\n",
    "\n",
    "        t = time() - tic\n",
    "        time_cumulative += t\n",
    "        \n",
    "        error = (test_m * (np.clip(pre, min_clip, max_clip) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
    "        test_rmse = np.sqrt(error)\n",
    "\n",
    "        error_train = (train_m * (np.clip(pre, min_clip, max_clip) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
    "        train_rmse = np.sqrt(error_train)\n",
    "\n",
    "        print('.-^-._' * 12)\n",
    "        print('PRE-TRAINING')\n",
    "        print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
    "        print('Time:', t, 'seconds')\n",
    "        print('Time cumulative:', time_cumulative, 'seconds')\n",
    "        print('.-^-._' * 12)\n",
    "\n",
    "    for i in range(epoch_f):\n",
    "        tic = time()\n",
    "        optimizer_f.minimize(sess, feed_dict={R: train_r})\n",
    "        pre = sess.run(pred_f, feed_dict={R: train_r})\n",
    "\n",
    "        t = time() - tic\n",
    "        time_cumulative += t\n",
    "        \n",
    "        error = (test_m * (np.clip(pre, min_clip, max_clip) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
    "        test_rmse = np.sqrt(error)\n",
    "\n",
    "        error_train = (train_m * (np.clip(pre, min_clip, max_clip) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
    "        train_rmse = np.sqrt(error_train)\n",
    "\n",
    "        test_mae = (test_m * np.abs(np.clip(pre, min_clip, max_clip) - test_r)).sum() / test_m.sum()\n",
    "        train_mae = (train_m * np.abs(np.clip(pre, min_clip, max_clip) - train_r)).sum() / train_m.sum()\n",
    "\n",
    "        test_ndcg = call_ndcg(np.clip(pre, min_clip, max_clip), test_r)\n",
    "        train_ndcg = call_ndcg(np.clip(pre, min_clip, max_clip), train_r)\n",
    "\n",
    "        if test_rmse < best_rmse:\n",
    "\n",
    "            best_rmse = test_rmse\n",
    "            best_rmse_ep = i+1\n",
    "            print(\"New best test rmse:\", best_rmse)\n",
    "\n",
    "            # Update predictions in the dataframe\n",
    "            test_samples['pred'] = np.clip(pre[test_samples['movie_id'], test_samples['user_id']], min_clip, max_clip)\n",
    "            train_samples['pred'] = np.clip(pre[train_samples['movie_id'], train_samples['user_id']], min_clip, max_clip)\n",
    "        \n",
    "        if test_mae < best_mae:\n",
    "            best_mae = test_mae\n",
    "            best_mae_ep = i+1\n",
    "\n",
    "        if best_ndcg < test_ndcg:\n",
    "            best_ndcg = test_ndcg\n",
    "            best_ndcg_ep = i+1\n",
    "\n",
    "        print('.-^-._' * 12)\n",
    "        print('FINE-TUNING')\n",
    "        print('Epoch:', i+1, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
    "        print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
    "        print('Time:', t, 'seconds')\n",
    "        print('Time cumulative:', time_cumulative, 'seconds')\n",
    "        print('.-^-._' * 12)\n",
    "\n",
    "    print('Best test rmse:', best_rmse, 'at epoch', best_rmse_ep)\n",
    "\n",
    "    # Rename movie_id to item_id\n",
    "train_samples = train_samples.rename(columns={'movie_id': 'item_id'})\n",
    "test_samples = test_samples.rename(columns={'movie_id': 'item_id'})\n",
    "\n",
    "    \n",
    "# Save the predictions in CSV files\n",
    "from save_model_outputs import save_model_outputs\n",
    "save_model_outputs(train_samples, test_samples, \"GLOCAL_K\", dataset, model_params )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTi_PdXJqTjh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55  best rmse: 0.41533613\n",
      "Epoch: 35  best mae: 0.20946513\n",
      "Epoch: 24  best ndcg: 0.976189879706521\n"
     ]
    }
   ],
   "source": [
    "# Final result\n",
    "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
    "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
    "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GLocal_K.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_GLOCAL_K",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
